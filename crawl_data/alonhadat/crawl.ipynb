{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from  lxml import etree\n",
    "import json \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import traceback\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME=5\n",
    "\n",
    "class crawling:\n",
    "    def __init__(self,):\n",
    "        self.home_page = \"https://alonhadat.com.vn/nha-dat/can-ban/nha-dat/1/ha-noi.html\"\n",
    "        self.root = \"https://alonhadat.com.vn\"\n",
    "        self.page = 1\n",
    "\n",
    "    def get_pages(self, url):\n",
    "        bs = BeautifulSoup(self.driver.page_source)\n",
    "        elements = bs.find_all(\"div\", {\"class\": \"ct_title\"})\n",
    "        if len(elements) == 0:\n",
    "            self.avoid_captcha(url)\n",
    "            bs = BeautifulSoup(self.driver.page_source)\n",
    "            elements = bs.find_all(\"div\", {\"class\": \"ct_title\"})\n",
    "        data = []\n",
    "        try:\n",
    "            for feature in elements:\n",
    "                feature = feature.find_all('a', href=True)\n",
    "                data.append(feature[0]['href'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return [self.root+link for link in data]\n",
    "    \n",
    "    def next_page(self):\n",
    "        self.page += 1\n",
    "        return self.home_page[:-5] + f\"/trang--{self.page}.html\"\n",
    "\n",
    "    def get_id(self, url):\n",
    "        bs = BeautifulSoup(self.driver.page_source)\n",
    "        page = etree.HTML(str(bs))\n",
    "        address = page.xpath(\"\"\"//*[@id=\"left\"]/div[1]/div[5]/div[2]/table/tbody/tr[1]\"\"\")\n",
    "        num = 5\n",
    "        while len(address) == 0 and num > 0:\n",
    "            self.avoid_captcha(url)\n",
    "            bs = BeautifulSoup(self.driver.page_source)\n",
    "            page = etree.HTML(str(bs))\n",
    "            address = page.xpath(\"\"\"//*[@id=\"left\"]/div[1]/div[5]/div[2]/table/tbody/tr[1]\"\"\")\n",
    "            num-=1\n",
    "        address = address[0]\n",
    "        return address.getchildren()[1].text\n",
    "    \n",
    "    def gather(self, page_source):\n",
    "        data = {}\n",
    "        bs = BeautifulSoup(page_source)\n",
    "        page = etree.HTML(str(bs))\n",
    "\n",
    "        # get price\n",
    "        price = page.xpath(\"\"\"//*[@id=\"left\"]/div[1]/div[3]/span[1]/span[2]\"\"\")[0].text\n",
    "        price = price.replace(\",\", \".\")\n",
    "        data['price'] = price\n",
    "\n",
    "        # get area\n",
    "        area = page.xpath(\"\"\"//*[@id=\"left\"]/div[1]/div[3]/span[2]/span[2]\"\"\")[0].text\n",
    "        data['area'] = area\n",
    "        \n",
    "        # get address\n",
    "        address = page.xpath(\"\"\"//*[@id=\"left\"]/div[1]/div[4]/span[2]\"\"\")[0].text\n",
    "        address = address.replace(\",\", \".\")\n",
    "        data['address'] = address\n",
    "\n",
    "        # get table information\n",
    "        table = bs.find_all('div', {'class': 'infor'})[0]\n",
    "        tr_instances = table.find_all('tr')\n",
    "        for i in range(len(tr_instances)):\n",
    "            td_instances = tr_instances[i].find_all('td')\n",
    "            for j in range(3):\n",
    "                data[str(td_instances[2*j].text).strip()] = str(td_instances[2*j+1].text)\n",
    "        \n",
    "        # get description\n",
    "        description = bs.find_all('div', {'class': 'detail text-content'})[0].text\n",
    "        data['description'] = description\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def avoid_captcha(self, page):\n",
    "        self.driver.close()\n",
    "        self.prepare_driver()\n",
    "        self.driver.get(page)\n",
    "        bs = BeautifulSoup(self.driver.page_source)\n",
    "        iframe_name = bs.find_all(\"iframe\", {\"title\": \"reCAPTCHA\"})[0]['name']\n",
    "        self.driver.switch_to.frame(iframe_name)\n",
    "        self.driver.find_element(By.XPATH, '''/html/body/div[2]/div[3]/div[1]/div/div/span''').click()\n",
    "        print('...')\n",
    "        time.sleep(20)\n",
    "        self.driver.switch_to.default_content()\n",
    "        self.driver.find_element(By.XPATH, '''/html/body/div[1]/form/button''').click()\n",
    "        print('success')\n",
    "\n",
    "    def prepare_driver(self):\n",
    "        service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "        options = webdriver.ChromeOptions() \n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        self.driver = webdriver.Chrome(options=options, service=service)\n",
    "        \n",
    "    def run(self, start, num_of_pages):\n",
    "        url = self.home_page\n",
    "        self.page = 1\n",
    "        if start != 1:\n",
    "            self.page = start\n",
    "            url = self.home_page[:-5] + f\"/trang--{self.page}.html\"\n",
    "        if os.path.exists(\"data/index.json\"):\n",
    "            with open(\"data/index.json\") as f:\n",
    "                index = set(json.load(f))\n",
    "        else:\n",
    "            index = set()\n",
    "        with open(\"log.txt\", \"w\") as f:\n",
    "            pass\n",
    "        dataset = []\n",
    "        for _ in range(num_of_pages):\n",
    "            num = 10\n",
    "            pages = []\n",
    "            while num > 0:\n",
    "                try:\n",
    "                    self.driver.get(url)\n",
    "                    # time.sleep(TIME)\n",
    "                    pages = self.get_pages(url)\n",
    "                    self.driver.delete_all_cookies()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    num -= 1\n",
    "            if num <= 0:\n",
    "                url = self.next_page()\n",
    "                continue\n",
    "            for page in tqdm(pages):\n",
    "                num = 1\n",
    "                while num > 0:\n",
    "                    try:\n",
    "                        self.driver.get(page)\n",
    "                        id = self.get_id(page)\n",
    "                        if id in index: \n",
    "                            print(id, \"exist\")\n",
    "                            break\n",
    "                        data = self.gather(self.driver.page_source)\n",
    "                        if data[\"Mã tin\"] not in index:\n",
    "                            dataset.append(data)\n",
    "                            index.add(data[\"Mã tin\"])\n",
    "                        with open(\"log.txt\", \"a\") as f:\n",
    "                            f.write(\"Success:\" + page +\"\\n\")\n",
    "                        self.driver.delete_all_cookies()\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e, \"unknown\")\n",
    "                        with open(\"log.txt\", \"a\") as f:\n",
    "                            f.write(f\"Excution{5-num}: {page}\"+\"\\n\") \n",
    "                            f.write(traceback.format_exc()+\"\\n\")\n",
    "                        with open(\"failed.txt\", \"a\") as f:\n",
    "                            f.write(f\"Excution{5-num}: {page}\"+\"\\n\") \n",
    "                            f.write(traceback.format_exc()+\"\\n\")\n",
    "                        self.driver.delete_all_cookies()\n",
    "                        num -= 1\n",
    "            url = self.next_page()\n",
    "        if len(dataset) != 0:\n",
    "            df = pd.DataFrame(dataset)\n",
    "            df.to_csv(f\"data/{start}_{start+num_of_pages}_{time.time()}.csv\", index=False)\n",
    "        with open(\"data/index.json\", \"w\") as f:\n",
    "            json.dump(list(index), f)\n",
    "        self.driver.delete_all_cookies()\n",
    "        \n",
    "craw = crawling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cần bán: 11353 (the last page)\n",
    "# Cho thuê: 1211 (the last page)\n",
    "craw.prepare_driver()\n",
    "for i in range(1, 11354):\n",
    "    craw.run(i, 1)\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5149034e6c270e282a77d1814557afd12f20860eafa66ad63ec09ade6ca502a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
